"""
AIRFLOW DAG DEMONSTRATION - Fraud Detection Project
=====================================================

This DAG demonstrates how the fraud detection reporting would work in Airflow.

NOTE: This is for project demonstration purposes. The actual daily reports 
are currently running via Windows Task Scheduler due to Airflow Windows 
compatibility issues.

For production deployment, this would run on Linux/Docker with full Airflow support.

DAG Structure:
1. Query fraud data from database
2. Generate comprehensive report  
3. Save report to logs
4. (Future: Send email notifications)

Schedule: Manual trigger only (no automatic schedule)
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
import logging

# DAG Configuration
default_args = {
    'owner': 'fraud-detection-team',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'start_date': days_ago(1),
}

# Create DAG
dag = DAG(
    'fraud_detection_manual_demo',
    default_args=default_args,
    description='Fraud Detection Manual Report (Ad Hoc)',
    schedule_interval=None,  # Manual trigger only - no automatic schedule
    catchup=False,
    tags=['fraud-detection', 'manual', 'ad-hoc'],
)

def extract_fraud_data(**context):
    """
    Task 1: Extract fraud data from the last 24 hours
    In production, this would connect to PostgreSQL database
    """
    logging.info("ðŸ” Extracting fraud data from database...")
    
    # This would be your actual database query
    mock_data = {
        'total_transactions': 715,
        'fraud_detected': 2,
        'legitimate_transactions': 713,
        'total_amount': 49493.65,
        'fraud_amount': 1702.18,
        'max_fraud_probability': 1.0000
    }
    
    # In Airflow, we'd use XCom to pass data between tasks
    logging.info(f"âœ… Extracted data: {mock_data}")
    return mock_data

def generate_fraud_report(**context):
    """
    Task 2: Generate comprehensive fraud report
    Uses data from previous task
    """
    logging.info("ðŸ“Š Generating fraud detection report...")
    
    # In Airflow, we'd pull data from previous task
    # data = context['task_instance'].xcom_pull(task_ids='extract_data')
    
    report_content = """
=========================================================
AIRFLOW FRAUD DETECTION REPORT - {date}
=========================================================

SYSTEM STATUS: âœ… OPERATIONAL
REPORT GENERATED BY: Apache Airflow DAG
SCHEDULE: Daily at 11:00 AM

RECENT ACTIVITY (Last 24 Hours):
â€¢ Transactions Processed: 715
â€¢ Legitimate: 713 (99.72%)
â€¢ Fraud Detected: 2 (0.28%)

FINANCIAL IMPACT:
â€¢ Total Volume: $49,493.65
â€¢ Fraud Blocked: $1,702.18
â€¢ Money Saved: $1,702.18

AIRFLOW DAG EXECUTION:
â€¢ DAG ID: daily_fraud_detection_report
â€¢ Task: generate_fraud_report
â€¢ Execution Date: {execution_date}

Generated by Apache Airflow
=========================================================
""".format(
        date=datetime.now().strftime('%Y-%m-%d'),
        execution_date=context.get('ds', 'N/A')
    )
    
    logging.info("ðŸ“ Report generated successfully")
    logging.info(report_content)
    
    return report_content

def save_report(**context):
    """
    Task 3: Save report to file system
    """
    logging.info("ðŸ’¾ Saving report to file system...")
    
    # In production, this would save to the actual logs directory
    # report = context['task_instance'].xcom_pull(task_ids='generate_report')
    
    filename = f"airflow_fraud_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    logging.info(f"âœ… Report would be saved as: logs/{filename}")
    
    return filename

def send_notifications(**context):
    """
    Task 4: Send notifications (Future enhancement)
    """
    logging.info("ðŸ“§ Sending notifications...")
    logging.info("âœ… Email notifications would be sent to fraud team")
    logging.info("âœ… Slack alerts would be posted to #fraud-detection")
    
    return "notifications_sent"

# Define task dependencies
extract_task = PythonOperator(
    task_id='extract_fraud_data',
    python_callable=extract_fraud_data,
    dag=dag,
)

generate_task = PythonOperator(
    task_id='generate_fraud_report', 
    python_callable=generate_fraud_report,
    dag=dag,
)

save_task = PythonOperator(
    task_id='save_report',
    python_callable=save_report,
    dag=dag,
)

notify_task = PythonOperator(
    task_id='send_notifications',
    python_callable=send_notifications,
    dag=dag,
)

# Set task dependencies (pipeline)
extract_task >> generate_task >> save_task >> notify_task

# DAG Documentation
dag.doc_md = """
# Daily Fraud Detection Report DAG

This DAG demonstrates a production-ready fraud detection reporting pipeline using Apache Airflow.

## Tasks:
1. **extract_fraud_data**: Query PostgreSQL for transaction data
2. **generate_fraud_report**: Create comprehensive fraud analysis report  
3. **save_report**: Store report in file system
4. **send_notifications**: Alert fraud team via email/Slack

## Schedule:
- **Frequency**: Daily at 11:00 AM
- **Catchup**: Disabled (won't backfill missed runs)
- **Retries**: 2 attempts with 5-minute delays

## Notes:
This DAG structure shows enterprise-level fraud detection automation.
For Windows compatibility, the actual reports currently run via Task Scheduler.
"""